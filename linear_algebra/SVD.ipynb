{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on the SVD (Singular Value Decomposition) #\n",
    "\n",
    "- Much of this comes from Trefethen's excellent book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A geometric interpretation ## \n",
    "\n",
    "We start by noting this important fact from Trefethen:\n",
    "\n",
    "\"The image of the unit sphere under any $m \\times n$ matrix is a hyperellipse.\n",
    "\n",
    "Let's unpack that statement:\n",
    "\n",
    "First, the *unit sphere* is the sphere in $\\mathbb { R } ^ { n}$, such that the Euclidian norm or $2$-norm is always $1$. \n",
    "\n",
    "A hyperellipse in $\\mathbb { R } ^ { m }$ is the $m$-dimensional generalization of an ellipse. We can obtain a hyperellipse by stretching the unit sphere in $\\mathbb { R } ^ { m}$ by factors $\\sigma_1,\\ldots,\\sigma_m$ in the orthogonal directions $\\mathbf{u}_1,\\ldots,\\mathbf{u}_m \\in \\mathbb { R } ^ { m}$ .\n",
    "\n",
    "For convenience, we take the $u_i$ to be unit vectors with $\\|u_i\\|_2 = 1$. The vectors $\\{\\sigma_i u_i\\}$ are then the *principal semiaxes* of the hyperellipse, each with length equal to $\\sigma_i$ (which could equal $0$). In particular, if the matrix $A$ has rank $r$, then exactly $r$ of these principal semiaxes will be nonzero.\n",
    "\n",
    "Let's now actually give a simple picture for this geometric interpretation. We denote the unit sphere in $n$-space as $S $. The hyperellipse in $m$-space, which is the image of $S$ under the mapping $A \\in \\mathbb{R}^{m\\times n}$, is then denoted simply as $AS$. A $2\\times 2$ example from Trefethen is show below:\n",
    "\n",
    "<img src=\"img/Trefethen_2by2_SVD.png\" width=\"600\" height=\"600\">\n",
    "\n",
    "We assume for simplicity that $A \\in \\mathbb{R}^{m\\times n}$ with $m \\geq n$ hase full rank $n$. The $n$ *singular values* $\\sigma_1,\\sigma_2,...,\\sigma_n$ of $A$ are then the lengths of the $n$ principal semiaxes of the hyperellipse $AS$. We usually number the singular values such that they are in descending order; thus, $\\sigma _ { 1 } \\geq \\sigma _ { 2 } \\geq \\cdots \\geq \\sigma_n > 0$.\n",
    "\n",
    "Next, we denote the $n$ unit vectors $\\left\\{ u _ { 1 } , u _ { 2 } , \\dots , u _ { n } \\right\\}$ representing the directions of the principal semiaxes to be the $n$ *left singular vectors* of A. These vectors are ordered to correspond with their axis lengths  $\\left\\{ \\sigma _ { 1 } , \\sigma _ { 2 } , \\dots , \\sigma _ { n } \\right\\}$. The vector $\\sigma_i u_i$ is thus the $i^{th}$ largest principal semiaxis of $AS$.\n",
    "\n",
    "Lastly, we have the $n$ *right singular vecotrs* of $A$, which are unit vectors $\\left\\{ v _ { 1 } , v _ { 2 } , \\ldots , v _ { n } \\right\\} \\in S$ representing the *preimages* of the principal semiaxes of $AS$. This then means that \n",
    "\n",
    "$$\n",
    "Av_i = \\sigma_iu_i \\quad 1 \\leq i \\leq n\n",
    "$$\n",
    "\n",
    "Putting this into a more compact matrix form, we arrive at the **Reduced SVD**, $A V = \\hat { U } \\hat { \\Sigma }$:\n",
    "\n",
    "<img src=\"img/Trefethen_reduced_SVD_matrices.png\" width=\"600\" height=\"600\">\n",
    "\n",
    "where $V$ is an $n\\times n$ matrix with orthonormal columns, $\\hat { U }$ is an $m \\times n$ matrix with orthonormal columns, and $\\hat { \\Sigma }$ is a diagonal $n \\times n$ matrix with the $\\sigma_i$ values on the diagonal. Since $V$ is unitary (the complex equivalent of orthogonal), we can multiply both sides on the right by $V^*$ to obtain\n",
    "\n",
    "$$\n",
    "A = \\hat { U } \\hat { \\Sigma } V ^ { * }\n",
    "$$\n",
    "\n",
    "The hats on the matrices signify that they are a par of the *reduced singular value decomposition*. This the form of the SVD factorization of $A$ we usually use in practice. \n",
    "\n",
    "We can turn this reduced SVD into the **Full SVD** by making $\\hat U$ into a unitary matrix $U$. To make $\\hat U$ unitary, it must be square and orthogonal, which we accomplish by appending $m-n$ orthonormal columns to $\\hat U$. \n",
    "\n",
    "To replace $\\hat U \\in \\mathbb{R}^{m\\times n}$ with $ U \\in \\mathbb{R}^{m \\times m}$, we must also require that the size of $\\hat { \\Sigma }$ is changed. Ideally, we want the $m-n$ columns we added to $\\hat U$ to have no effect on our product. Thus, we add $m-n$ zero rows to $\\hat { \\Sigma } \\in \\mathbb{R}^{n\\times n}$ to make ${ \\Sigma } \\in \\mathbb{R}^{m\\times n}$. Thus, we arrive at the full SVD\n",
    "\n",
    "$$\n",
    "A = { U } { \\Sigma } V ^ { * }\n",
    "$$\n",
    "\n",
    "where $U$ is $m\\times m$ and unitary, $V$ is $n\\times n$ and unitary, and $\\Sigma$ is $m\\times n$ and diagonal with only positive real entries. Let's compare the two approaches pictorally.\n",
    "\n",
    "<img src=\"img/Trefethen_reduced_SVD_sizes.png\" width=\"400\" height=\"400\">\n",
    "\n",
    "<img src=\"img/Trefethen_full_SVD_sizes.png\" width=\"400\" height=\"400\">\n",
    "\n",
    "As a final note, it is worth mentioning that $A$ need not actually be full rank. If the rank of $A$ is $r < \\min(n,m)$, then $\\Sigma$ will only have $r$ positive entries on the diagonal, with the other diagonal entries equal to $0$. We stil fill out the columns of $U$ and rows of $\\Sigma$, but now with $m-r$ columns and rows, respectively. One additional note is that we must now add $n-r$ orthonormal columns to $V$ in order to fill out the $r$ columns dictated by the geometry.\n",
    "\n",
    "One last interpretation, let's imagine $A$ acting on $S$. And let's break the multiplication up like so:\n",
    "\n",
    "$$\n",
    "AS = ({ U } ({ \\Sigma } (V ^ { * }S)))\n",
    "$$\n",
    "\n",
    "First, the unitary matrix $V ^ { * }$ preserves the sphere. The $\\Sigma$ acts to stretches the sphere in directions aligned with the canonical basis vectors, with lengths corresponding to the singular values. This produces a hyperellipse. Lastly, the unitary matrix $U$ rotates or reflects the the hyperellipse into our final hyperellipse.\n",
    "\n",
    "### Existence and Uniqueness ###\n",
    "\n",
    "See Trefethen for a proof that every matrix $A \\in \\mathbb { C } ^ { m \\times n }$ has an SVD, and that the singular values are uniquely determined.\n",
    "\n",
    "\n",
    "## Cool Facts about the SVD ##\n",
    "\n",
    "**Theorem 5.1 (Trefethen):** *The rank of $A$ is $r$, the number of nonzero singular values*\n",
    "\n",
    "First we should recall the theorem that $\\operatorname { Rank } ( A B ) \\leq \\min ( \\operatorname { Rank } ( A ) , \\operatorname { Rank } ( B ) )$. The intuition for this is to note that the $i^{th}$ column of $AB$ is $A$ times the $i^{th}$ column of $B$. Therefore, all columns of $AB$ lie in the column space of $A$. The dimension of the column space (the rank) of $AB$ is thus *at most* the same as the rank of $A$. The rank could only decrease if $B$ has a lower rank than $A$.\n",
    "\n",
    "Now back to the SVD decomposition $A = { U } { \\Sigma } V ^ { * }$. We know that $U$ and $V$ are full rank; therefore, the rank of $A$ is limited by ${ \\Sigma }$. Furthermore, the rank of any diagonal matrix is just the number of nonzero entries. Therfore, $\\operatorname { rank } ( A ) = \\operatorname { rank } ( \\Sigma ) = r$.\n",
    "\n",
    "**Theorem 5.3 (Trefethen):** $\\| A \\| _ { 2 } = \\sigma _ { 1 }$ and $\\| A \\| _ { F } = \\sqrt { \\sigma _ { 1 } ^ { 2 } + \\sigma _ { 2 } ^ { 2 } + \\cdots + \\sigma _ { r } ^ { 2 } }$\n",
    "\n",
    "For $A \\in \\mathbb { C } ^ { m \\times n } $, we start with the definition of the Frobenius norm:\n",
    "\n",
    "$$\n",
    "\\| A \\| _ { F } = \\left( \\sum _ { i = 1 } ^ { m } \\sum _ { j = 1 } ^ { n } \\left| a _ { i j } \\right| ^ { 2 } \\right) ^ { 1 / 2 }\n",
    "$$\n",
    "\n",
    "We now note that the Frobenius norm can be written more simply in terms of the trace:\n",
    "\n",
    "$$\n",
    "\\| A \\| _ { F } = \\sqrt { \\operatorname { tr } \\left( A ^ { * } A \\right) } = \\sqrt { \\operatorname { tr } \\left( A A ^ { * } \\right) }\n",
    "$$\n",
    "\n",
    "To see this equality, consider first the quantity $\\operatorname { tr } \\left( A ^ { * } A \\right)$. The element $(A ^ { * } A)_{ii}$ is just the dot product of the $i^{th}$ column of $A$ with itself, ($A_{:,i} \\cdot A_{:,i}$).\n",
    "\n",
    "Alternatively, the element $(A  A^ { * })_{ii}$ is just the dot product of the $i^{th}$ row of $A$ with itself, ($A_{i,:} \\cdot A_{i,:}$). With a little thought, it should be apparent that $\\sum_i (A ^ { * } A)_{ii} = \\sum_i (A  A^ { * })_{ii} = \\left(\\| A \\| _ { F }\\right)^2$.\n",
    "\n",
    "This formulation of the Frobenius norm in terms of the trace makes it easy to see some specific properties of this norm. For example, observe the effect of multiplication a unitary matrix $Q \\in \\mathbb { C } ^ { m \\times m }$ on $A$.\n",
    "\n",
    "$$\n",
    "\\| Q A \\| _ { F } = \\sqrt { \\operatorname { tr } \\left( (QA) ^ { * } QA \\right) } = \\sqrt { \\operatorname { tr } \\left( A^ { * }Q^ { * } QA \\right) } = \\sqrt { \\operatorname { tr } \\left( A ^ { * } A \\right) } = \\| A \\| _ { F }\n",
    "$$\n",
    "\n",
    "Thus, the Frobenius norm is invariant under multplication by unitary matrices. \n",
    "\n",
    "Finally, we return to the SVD, $A = { U } { \\Sigma } V ^ { * }$. Since $U$ and $V$ are unitary, $\\| A \\| _ { F }=\\| \\Sigma \\| _ { F } = \\sqrt { \\sigma _ { 1 } ^ { 2 } + \\sigma _ { 2 } ^ { 2 } + \\cdots + \\sigma _ { r } ^ { 2 } }$\n",
    "\n",
    "**Low Rank Approximations** (see Trefethen Theorem 5.8 for more detail)\n",
    "\n",
    "A nice result is that $A$ can be written as the sum of $r$ rank-one matrices like so\n",
    "\n",
    "$$\n",
    "A = \\sum _ { j = 1 } ^ { r } \\sigma _ { j } u _ { j } v _ { j } ^ { * }\n",
    "$$\n",
    "\n",
    "We now define the $\\nu^{th}$ partial sum for any $\\nu$ with $0 \\leq \\nu \\leq r$ as\n",
    "\n",
    "$$\n",
    "A _ { \\nu } = \\sum _ { j = 1 } ^ { \\nu } \\sigma _ { j } u _ { j } v _ { j } ^ { * }\n",
    "$$\n",
    "\n",
    "It turns out that the $\\nu^{th}$ partial sum $A_{\\nu}$of $A$ is the best approximation to $A$ with rank $\\leq \\nu$ in terms of the Frobenius norm. More formally, we can write:\n",
    "\n",
    "$$\n",
    "\\left\\| A - A _ { \\nu } \\right\\| _ { F } = \\inf _ { B \\in \\mathbb { C } ^ { m \\times n } \\atop \\operatorname { rank } ( B ) \\leq \\nu } \\| A - B \\| _ { F } = \\sqrt { \\sigma _ { \\nu + 1 } ^ { 2 } + \\cdots + \\sigma _ { r } ^ { 2 } }\n",
    "$$\n",
    "\n",
    "As one can see, if the first $\\nu$ singular values of $A$ are large relative to the remaining singular values, then a low-rank approximation can capture much of the Frobenius norm.\n",
    "\n",
    "**Connection to eigenvectors and eigenvalues**\n",
    "\n",
    "**Theorem 5.4 (Trefethen):** *The nonzero singular values of $A$ are the square roots of the nonzero eigenvalues of $A ^ { * } A$ or $A A ^ { * }$*\n",
    "\n",
    "The proof for either $A ^ { * } A$ or $A A ^ { * }$ is quite simple, we'll just start with one of them:\n",
    "\n",
    "$$\n",
    "A ^ { * } A = \\left( U \\Sigma V ^ { * } \\right) ^ { * } \\left( U \\Sigma V ^ { * } \\right) = V \\Sigma ^ { * } U ^ { * } U \\Sigma V ^ { * } = V \\left( \\Sigma ^ { * } \\Sigma \\right) V ^ { * }\n",
    "$$\n",
    "\n",
    "Thus we see that  $A ^ { * } A$ is similar to $\\Sigma ^ { * } \\Sigma$, and therefore they share the same eigenvalues. The eigenvalues of a diagonal matrix are just the diagonal entries, thus the eigenvalues of $\\Sigma ^ { * } \\Sigma$ are $\\sigma _ { 1 } ^ { 2 } , \\sigma _ { 2 } ^ { 2 } , \\ldots , \\sigma _ { r } ^ { 2 }$.\n",
    "\n",
    "The same proof for $A A ^ { * }$ is also quite simple.\n",
    "\n",
    "$$\n",
    " AA ^ { * } =  \\left( U \\Sigma V ^ { * } \\right)\\left( U \\Sigma V ^ { * } \\right) ^ { * } =  U \\Sigma V ^ { * }V \\Sigma ^ { * } U ^ { * } = U \\left( \\Sigma \\Sigma ^ { * } \\right) U ^ { * }\n",
    "$$\n",
    "\n",
    "Both of these proofs also reveal something interesting about the eigenvectors of $A ^ { * } A$ and $A A ^ { * }$. The columns of $V$, the *right singular vectors*, are the eigenvectors of $A ^ { * }A$, while the columns of $U$, the *left singular vectors*, are the eigenvectors of $AA ^ { * }$.\n",
    "\n",
    "One can also easily prove that if $A$ is symmetric, then the singular values of $A$ are the absolute values of the eigenvalues of $A$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
