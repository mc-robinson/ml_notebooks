{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Methods #\n",
    "    - Matt Robinson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobi's Method:\n",
    "\n",
    "We start with our square system of $n$ linear equations\n",
    "\n",
    "$$\n",
    "\\mathbf{A}\\mathbf{x} = \\mathbf{b}\n",
    "$$\n",
    "\n",
    "We then decompose $\\mathbf{A}$ into the sum of strictly lower triangular matrix $\\mathbf{L}$, a strictly diagonal matrix $\\mathbf{D}$, and a strictly upper triangular matrix $\\mathbf{U}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\mathbf{L} + \\mathbf{D} + \\mathbf{U}\n",
    "$$\n",
    "\n",
    "Using this decomposition, we can now write our system as follows:\n",
    "\n",
    "$$\n",
    "(\\mathbf{L} + \\mathbf{D} + \\mathbf{U})\\mathbf{x} = \\mathbf{b} \\\\\n",
    "\\implies \\mathbf{D}\\mathbf{x} = -(\\mathbf{L} + \\mathbf{U})\\mathbf{x} + \\mathbf{b}\\\\\n",
    "$$\n",
    "\n",
    "We then solve for the solution through an iterative scheme:\n",
    "\n",
    "$$\n",
    "\\mathbf{D}\\mathbf{x}^{(k+1)} = -(\\mathbf{L} + \\mathbf{U})\\mathbf{x}^{(k)} + \\mathbf{b}\\\\\n",
    "\\implies \\mathbf{x}^{(k+1)} = \\mathbf{D}^{-1} \\left(\\mathbf{b} -(\\mathbf{L} + \\mathbf{U})\\mathbf{x}^{(k)}\\right)\\\\\n",
    "$$\n",
    "\n",
    "Note that this shows that the iteration matrix $\\mathbf{H}_J = \\mathbf{D}^{-1}(\\mathbf{L} + \\mathbf{U})$. Futhermore, the above formula can be expressed elementwise to get:\n",
    "\n",
    "$$\n",
    "x _ { i } ^ { ( k + 1 ) } = \\frac { 1 } { A _ { i i } } \\left( b _ { i } - \\sum _ { j \\neq i } A _ { i j } x _ { j } ^ { ( k ) } \\right) , \\quad i = 1,2 , \\ldots , n\n",
    "$$\n",
    "\n",
    "Many books will justify this formula more simply by starting with the elementwise form of $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$ and separating out the diagonal components. For example, see the following argument copied from Professor Gerard Gorman's [numerical analysis notes](https://nbviewer.jupyter.org/github/ggorman/Numerical-methods-1/blob/master/notebook/Lecture-7-Numerical-methods-1.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"\"\n",
    "### ggorman's element-wise explanation:\n",
    "Copied from link above.\n",
    "\n",
    "Consider our matrix system\n",
    "\n",
    "$$A\\pmb{x}=\\pmb{b} \\quad \\iff \\quad \\sum_{j=1}^nA_{ij}x_j=b_i,\\quad \\textrm{for}\\quad i=1,2,\\ldots, n.$$\n",
    "\n",
    "Let's rewrite this by pulling out the term involving $x_i$ (i.e. for each row $i$ pull out the diagonal from the summation):\n",
    "\n",
    "$$A_{ii}x_i + \\sum_{\\substack{j=1\\\\ j\\ne i}}^nA_{ij}x_j=b_i,\\quad  i=1,2,\\ldots, n.$$\n",
    "\n",
    "We can then come up with a formula for our unknown $x_i$:\n",
    "\n",
    "$$x_i = \\frac{1}{A_{ii}}\\left(b_i- \\sum_{\\substack{j=1\\\\ j\\ne i}}^nA_{ij}x_j\\right),\\quad  i=1,2,\\ldots, n.$$\n",
    "\n",
    "Now of course for each individual $x_i$, all the other components of $\\pmb{x}$ appearing on the RHS are also unknown and so this is an example of an implicit formula which doesn't help us directly, but does suggest the following iterative scheme:\n",
    "\n",
    "* Starting from a guess at the solution $\\pmb{x}^{(0)}$\n",
    "\n",
    "* iterate for $k>0$\n",
    "$$x_i^{(k+1)} = \\frac{1}{A_{ii}}\\left(b_i- \\sum_{\\substack{j=1\\\\ j\\ne i}}^nA_{ij}x_j^{(k)}\\right),\\quad  i=1,2,\\ldots, n.$$\n",
    "\n",
    "Note that for this iteration, for a fixed $k$, it does not matter in which order we perform the operations over $i$ as the right hand side only contains the entries of $\\pmb{x}$ at the previous iteration.\n",
    "\n",
    "### end of ggorman's explanation\n",
    "# \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jacobi(A, b, tol=1.0E-6, max_iter=100):\n",
    "    \n",
    "    # initial guess at the solution - just use zero vector for now\n",
    "    x = np.zeros(A.shape[1])\n",
    "    # inital x^{k+1} vector\n",
    "    x_new = np.empty(A.shape[1])\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        \n",
    "        D = np.diag(np.diag(A)) \n",
    "        L_plus_U = A - D\n",
    "        \n",
    "        x_new = np.linalg.inv(D).dot(b - L_plus_U.dot(x))\n",
    "        \n",
    "        error = np.linalg.norm(A.dot(x_new) - b)\n",
    "        if error < tol:\n",
    "            return x_new\n",
    "        else:\n",
    "            x = x_new\n",
    "    \n",
    "    raise RuntimeError('no root found in allowed number of iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Jacobi Method:  [-0.16340807 -0.01532701  0.27335259  0.36893548]\n",
      "Numpy:  [-0.16340816 -0.01532706  0.27335264  0.36893555]\n"
     ]
    }
   ],
   "source": [
    "A= np.array([[10., 2., 3., 5.],\n",
    "             [1., 14., 6., 2.],\n",
    "             [-1., 4., 16., -4],\n",
    "             [5. ,4. ,3. ,11. ]])\n",
    "\n",
    "b = np.array([1., 2., 3., 4.])\n",
    "\n",
    "print(\"Our Jacobi Method: \", Jacobi(A,b))\n",
    "print(\"Numpy: \", np.dot(np.linalg.inv(A),b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gauss-Seidel Method ##\n",
    "\n",
    "The element-wise formulation of Jacobi's method shows that we only use the components of the last iteration $x _ { i } ^ { ( k ) }$ in constructing $x _ { i } ^ { ( k + 1 ) }$. Naturally, one wonders what would happen if we use the updated components of the solution vector $x _ { i } ^ { ( k + 1 ) }$ as soon they become available. That is, a component $x _ { j } ^ { ( k + 1 ) }$ might rely on $x _ { i } ^ { ( k + 1 ) }$ instead of $x _ { i } ^ { ( k ) }$. This improvement to the Jacobi method is known as the *Gauss-Seidel* Method, which is often faster than Jacobi.\n",
    "\n",
    "$$\n",
    "x _ { i } ^ { ( k+1 ) } = \\frac { 1 } { A _ { i i } } \\left( b _ { i } - \\sum _ { j = 1 \\atop j<i} ^ { n } A _ { i j } x _ { j } ^ { ( k+1 ) } - \\sum _ { j = 1 \\atop j > i } ^ { n } A _ { i j } x _ { j } ^ { ( k ) } \\right) , \\quad i = 1,2 , \\ldots , n\n",
    "$$\n",
    "\n",
    "Let's now look at the matrix formulation of the Gauss-Seidel method. We start again by decomposing $\\mathbf{A}$:\n",
    "\n",
    "$$\n",
    "(\\mathbf{L} + \\mathbf{D} + \\mathbf{U})\\mathbf{x} = \\mathbf{b} \\\\\n",
    "\\implies (\\mathbf{L} + \\mathbf{D})\\mathbf{x}^{(k+1)} = - \\mathbf{U}\\mathbf{x}^{(k)} + \\mathbf{b}\\\\\n",
    "$$\n",
    "\n",
    "We see that our iteration matrix could then be calculated as $\\mathbf{H} _ { G S } = - ( \\mathbf{L} + \\mathbf{D} ) ^ { - 1 } \\mathbf{U}$. However, in practice, there is no need for us to invert $( \\mathbf{L} + \\mathbf{D} )$. Since $( \\mathbf{L} + \\mathbf{D} )$ is a strictly lower triangular matrix (does not include the diagonal) plus a diagonal matrix, we can simply use forward substitution to solve this system. Using forward substitution leads to the exact same element-wise formula as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GS(A, b, tol=1.0E-6, max_iter=100):\n",
    "    \n",
    "    # initial guess at the solution - just use zero vector for now\n",
    "    x = np.zeros(A.shape[1])\n",
    "    # inital x^{k+1} vector\n",
    "    x_new = np.empty(A.shape[1])\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        \n",
    "        for i in range(len(x_new)):\n",
    "            x_new[i] = (1/A[i, i]) * (b[i] - np.dot(A[i, :i], x_new[:i]) - np.dot(A[i, i + 1:], x[i + 1:]))\n",
    "        \n",
    "        error = np.linalg.norm(A.dot(x_new) - b)\n",
    "        if error < tol:\n",
    "            return x_new\n",
    "        else:\n",
    "            x = x_new\n",
    "    \n",
    "    raise RuntimeError('no root found in allowed number of iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Gauss-Seidel Method:  [-0.16340812 -0.01532701  0.27335261  0.36893553]\n",
      "Numpy:  [-0.16340816 -0.01532706  0.27335264  0.36893555]\n"
     ]
    }
   ],
   "source": [
    "A= np.array([[10., 2., 3., 5.],\n",
    "             [1., 14., 6., 2.],\n",
    "             [-1., 4., 16., -4],\n",
    "             [5. ,4. ,3. ,11. ]])\n",
    "\n",
    "b = np.array([1., 2., 3., 4.])\n",
    "\n",
    "print(\"Our Gauss-Seidel Method: \", GS(A,b))\n",
    "print(\"Numpy: \", np.dot(np.linalg.inv(A),b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When do these methods work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy:  [2.33333333 1.33333333]\n"
     ]
    }
   ],
   "source": [
    "# a quick counter example\n",
    "A= np.array([[1, 2],\n",
    "             [2, 1]])\n",
    "\n",
    "b = np.array([5, 6])\n",
    "print(\"Numpy: \", np.dot(np.linalg.inv(A),b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "no root found in allowed number of iterations",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-06a2c43dd29d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Our Jacobi Method: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJacobi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-d0d63baed5ec>\u001b[0m in \u001b[0;36mJacobi\u001b[0;34m(A, b, tol, max_iter)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'no root found in allowed number of iterations'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: no root found in allowed number of iterations"
     ]
    }
   ],
   "source": [
    "print(\"Our Jacobi Method: \", Jacobi(A,b,max_iter=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "no root found in allowed number of iterations",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-d662e566fb89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Our Gauss-Seidel Method: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-d626e207c348>\u001b[0m in \u001b[0;36mGS\u001b[0;34m(A, b, tol, max_iter)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'no root found in allowed number of iterations'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: no root found in allowed number of iterations"
     ]
    }
   ],
   "source": [
    "print(\"Our Gauss-Seidel Method: \", GS(A,b,max_iter=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously these methods don't work for every system. Let's go through some conditions for their success.\n",
    "\n",
    "**FACT**: The sequence converges to the solution if the spectral radius (max absolute value of eigenvalues) of the iteration matrix is less than one. For Jacobi, the iteration matrix is $\\mathbf{H}_J = \\mathbf{D}^{-1}(\\mathbf{L} + \\mathbf{U})$, while for Gauss-Seidel the iteration matrix is $\\mathbf{H} _ { G S } = - ( \\mathbf{L} + \\mathbf{D} ) ^ { - 1 } \\mathbf{U}$. \n",
    "\n",
    "One can prove that the spectral radius of these matrices is less than one for special classes of matrices, and thus we can discover under which classes of matrices our methods will converge.\n",
    "\n",
    "**Theorem**: If $\\mathbf{A}$ is strictly *diagonally dominant*, then both Jacobi and Gauss-Seidel will converge. A matrix is defined as strictly diagonally dominant if, for every row, the magnitude of the diagonal element is greater than the sum of all of the other elements in the row. That is,\n",
    "\n",
    "$$\n",
    "\\left| A _ { i , i } \\right| > \\sum _ { j = 1 \\atop j \\neq i } ^ { n } \\left| A _ { i , j } \\right| \\quad \\text{for all } i\n",
    "$$\n",
    "\n",
    "**Theorem**: If $\\mathbf{A}$ is symmetric and positive definite, then the Gauss-Seidel method will converge.\n",
    "\n",
    "Note that this does not necessarily guarentee the convergence of Jacobi. In this case, $\\mathbf{A}$ and $2\\mathbf{D} -\\mathbf{A}$ must both be symmetric and positive definite to guarentee convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ggorman\n",
    "def is_diag_dom(A):\n",
    "        \n",
    "    (n,m) = A.shape\n",
    "    if n != m: \n",
    "        print(\"Error: matrix must be square, not %dx%d\" % (n,m))\n",
    "        exit(1)\n",
    "    \n",
    "    for i in range(n):\n",
    "        abs_row = np.sum(np.abs(A[i,0:i])) + np.sum(np.abs(A[i,i+1:n]))\n",
    "        if abs_row > abs(A[i,i]):\n",
    "            print(\"!! Row %d is not diagonally dominant\" %i)\n",
    "            return False\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below I am writing Gauss-Seidel with relaxation\n",
    "\n",
    "def relaxed_GS(A, b, omega=1, tol=1.0E-6, max_iter=1000):\n",
    "    \n",
    "    # initial guess at the solution - just use zero vector for now\n",
    "    x = np.zeros(A.shape[1])\n",
    "    # inital x^{k+1} vector\n",
    "    x_new = np.empty(A.shape[1])\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        \n",
    "        for i in range(len(x_new)):\n",
    "            x_new[i] = ( (omega/A[i, i]) * (b[i] - np.dot(A[i, :i], x_new[:i]) - np.dot(A[i, i + 1:], x[i + 1:])) \n",
    "                        + (1-omega)*x[i] )\n",
    "        \n",
    "        error = np.linalg.norm(A.dot(x_new) - b)\n",
    "        if error < tol:\n",
    "            return x_new, iteration\n",
    "        else:\n",
    "            x = x_new\n",
    "    \n",
    "    raise RuntimeError('no root found in allowed number of iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!! Row 3 is not diagonally dominant\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[10., 2., 3., 5.],\n",
    "                [1., 14., 6., 2.],\n",
    "                 [-1., 4., 16.,-4],\n",
    "                 [5. ,4. ,3. ,11.]])\n",
    "b = np.array([1., 2., 3., 4.])\n",
    "\n",
    "print(is_diag_dom(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For omega=0.50  48 relaxed GS iterations are needed to reach a tolerance of 1e-6\n",
      "For omega=0.90  18 relaxed GS iterations are needed to reach a tolerance of 1e-6\n",
      "For omega=0.95  16 relaxed GS iterations are needed to reach a tolerance of 1e-6\n",
      "For omega=1.00  14 relaxed GS iterations are needed to reach a tolerance of 1e-6\n",
      "For omega=1.10  12 relaxed GS iterations are needed to reach a tolerance of 1e-6\n",
      "For omega=1.50  29 relaxed GS iterations are needed to reach a tolerance of 1e-6\n"
     ]
    }
   ],
   "source": [
    "for omega in [0.5, 0.9, 0.95, 1, 1.1, 1.5]:\n",
    "    x,i = relaxed_GS(A, b, omega)\n",
    "    print(\"For omega=%1.2f  %d relaxed GS iterations are needed to reach a tolerance of 1e-6\" % (omega,i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steepest Descent and Conjugate Gradient Methods #\n",
    "\n",
    "Here I will mostly just supply the code, see Faul's notes for more details.\n",
    "\n",
    "Note that $\\mathbf{A}$ must be symmetric and positive definite for the method to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SD(A, b, tol=1.0E-6, max_iter=1000):\n",
    "    \n",
    "    if not np.array_equal(A,A.T):\n",
    "        raise ValueError('A is not symmetric')\n",
    "    \n",
    "    for e_val in np.linalg.eigvals(A):\n",
    "        if e_val <= 0:\n",
    "            raise ValueError('A is not positive definite') \n",
    "    \n",
    "    # initial guess at the solution - just use zero vector for now\n",
    "    x = np.zeros(A.shape[1])\n",
    "    # inital x^{k+1} vector\n",
    "    x_new = np.empty(A.shape[1])\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        \n",
    "        # calculate gradient, search direction, and step size\n",
    "        grad = A.dot(x) - b\n",
    "        search_direction = -1*grad\n",
    "        omega = (grad.dot(grad))/(grad.dot(A.dot(grad)))\n",
    "        \n",
    "        x_new = x + omega*search_direction\n",
    "        \n",
    "        gradient_norm = np.linalg.norm(A.dot(x_new) - b)\n",
    "        if gradient_norm < tol:\n",
    "            return x_new, iteration\n",
    "        else:\n",
    "            x = x_new\n",
    "    \n",
    "    raise RuntimeError('No root found in allowed number of iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Steepest Descent Method:  [2.49999962 3.99999923 3.49999962]\n",
      "Numpy:  [2.5 4.  3.5]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[2, -1., 0],\n",
    "              [-1., 2, -1.],\n",
    "              [0., -1., 2]])\n",
    "b = np.array([1., 2., 3.])\n",
    "\n",
    "x, num_iters = SD(A,b)\n",
    "print(\"Our Steepest Descent Method: \", x)\n",
    "print(\"Numpy: \", np.dot(np.linalg.inv(A),b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's code the conjugate gradient method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CG(A, b, tol=1.0E-6, max_iter=8):\n",
    "    '''\n",
    "    See http://www.acme.byu.edu/wp-content/uploads/2014/09/Vol2Lab18ConjugateGradient.pdf\n",
    "    for correct description of algorithm implementation\n",
    "    '''\n",
    "    \n",
    "    if not np.array_equal(A,A.T):\n",
    "        raise ValueError('A is not symmetric')\n",
    "    \n",
    "    for e_val in np.linalg.eigvals(A):\n",
    "        if e_val <= 0:\n",
    "            raise ValueError('A is not positive definite') \n",
    "    \n",
    "    # initial guess at the solution - just use 0 vector for now\n",
    "    x = np.zeros(A.shape[1])\n",
    "    # intialize the residual = -1*gradient. Initially this is b, since x0 is 0 vector\n",
    "    r = A.dot(x)-b\n",
    "    # initialize search direction to -1*gradient = residual\n",
    "    d = -1*r\n",
    "    \n",
    "    # inital {k+1} vectors\n",
    "    x_new = np.empty(len(x))\n",
    "    r_new = np.empty(len(r))\n",
    "    d_new = np.empty(len(d))\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        \n",
    "        # calculate gradient, search direction, and step size\n",
    "        omega = (r.dot(r))/(d.dot(A.dot(d)))\n",
    "        \n",
    "        x_new = x + omega*d\n",
    "        r_new = r + omega*(A.dot(d))\n",
    "        \n",
    "        # print(x_new)\n",
    "        \n",
    "        beta = (r_new.dot(r_new))/(r.dot(r))\n",
    "        d_new = -r_new + beta*d\n",
    "        \n",
    "        if np.linalg.norm(r_new) < tol:\n",
    "            return x_new, iteration\n",
    "        else:\n",
    "            x = x_new\n",
    "            r = r_new\n",
    "            d = d_new\n",
    "    \n",
    "    raise RuntimeError('No root found in allowed number of iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Conjugate Gradient Method:  [2.5 4.  3.5]\n",
      "Numer of Iterations:  2\n",
      "Numpy:  [2.5 4.  3.5]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[2, -1., 0],\n",
    "              [-1., 2, -1.],\n",
    "              [0., -1., 2]])\n",
    "b = np.array([1., 2., 3.])\n",
    "\n",
    "\n",
    "x, num_iters = CG(A,b)\n",
    "print(\"Our Conjugate Gradient Method: \", x)\n",
    "print(\"Numer of Iterations: \", num_iters)\n",
    "print(\"Numpy: \", np.dot(np.linalg.inv(A),b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_method(A, tol=1.0E-3, max_iter=100):\n",
    "    \n",
    "    if not np.array_equal(A,A.T):\n",
    "        raise ValueError('A is not symmetric')\n",
    "    \n",
    "    # initial guess - just use ones vector\n",
    "    # then normalize in order to make it a unit vector\n",
    "    x = np.ones(A.shape[1])/np.linalg.norm(np.ones(A.shape[1]))\n",
    "    \n",
    "    # inital x^{k+1} vector\n",
    "    x_new = np.empty(A.shape[1])\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        \n",
    "        x_new = A.dot(x)\n",
    "        min_lambda = (x.dot(A.dot(x)))/(x.dot(x))\n",
    "        \n",
    "        error = np.linalg.norm(x_new - min_lambda*x)\n",
    "        if error < tol:\n",
    "            return x_new/np.linalg.norm(x_new), min_lambda, iteration\n",
    "        else:\n",
    "            x = x_new/np.linalg.norm(x_new)\n",
    "    \n",
    "    raise RuntimeError('no root found in allowed number of iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power Method Eigenvector:  [ 0.50001275 -0.70708874  0.50001275]\n",
      "Numpy Eigenvector:  [-0.5        -0.70710678  0.5       ] \n",
      "\n",
      "Power Method Eigenvalue:  3.4142134998513236\n",
      "Numpy Eigenvalue:  3.4142135623730914\n"
     ]
    }
   ],
   "source": [
    "e_vec, e_val, num_iters = power_method(A)\n",
    "print(\"Power Method Eigenvector: \",e_vec)\n",
    "print(\"Numpy Eigenvector: \", np.linalg.eig(A)[1][0],\"\\n\")\n",
    "\n",
    "print(\"Power Method Eigenvalue: \",e_val)\n",
    "print(\"Numpy Eigenvalue: \", np.linalg.eigvals(A)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
