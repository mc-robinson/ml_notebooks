{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA & QDA From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA\n",
    "\n",
    "LDA (Linear Discriminent Analysis) is a **supervised** learning technique that can be used for classification and/or dimensionality reduction. \n",
    "\n",
    "There seem to be two main ways to introduce LDA, which correspond to Fisher's (1936) original approach and Welch's (1939) later derivation. I'll start by considering Fisher's approach, which seems to avoid (or at least hides?) many of the assumptions. \n",
    "\n",
    "A lot of peeople introduce LDA by likening it to PCA, an unsupervised dimensionality reduction algorithm for finding the directions of greatest variance in a dataset. The main difference is that LDA uses the class labels (thus making it supervised learning) to find the directions (or combinations of attributes, if you prefer) that give the greatest separation *between classes*. \n",
    "\n",
    "Let's consider the simple case of two classes, and an $n$-dimensional input vector $\\mathbf{x}$. We hope to project that vector down to one dimension using $y=\\mathbf{w}^T \\mathbf{x}$ with the appropriate weights $\\mathbf{w}$, which we hope to find. But how do we find this projection/direction that maximizes separability?\n",
    "\n",
    "One way to achieve this is by maximizing the following quanitity:\n",
    "\n",
    "$$\n",
    "\\frac{\\text{Variance Between Classes}}{\\text{Variance Within Classes}}\n",
    "$$\n",
    "\n",
    "So let's find expressions for the numerator and the denominator. To write these variances, we will make use of the covariance matrix.\n",
    "\n",
    "To set things up, let's say I have a class $C_i$. To find the covariance matrix, I will require the sample mean over all features for the samples in $C_i$, which is an $n\\times 1$ matrix:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\mu}_i = \\frac{1}{N_i}\\sum_{\\mathbf{x} \\in C_i} \\mathbf{x}\n",
    "$$\n",
    "\n",
    "The sample covariance matrix for the class $C_i$ can then be defined succinctly as an outer product:\n",
    "\n",
    "$$\n",
    "S_i = \\frac{1}{N_i}\\sum_{\\mathbf{x} \\in C_i} (\\mathbf{x} - \\mathbf{\\mu_i})(\\mathbf{x} - \\mathbf{\\mu_i})^T\n",
    "$$\n",
    "\n",
    "which is a symmetric, positive definite, $n\\times n$ matrix. The $j,j$ entry of $S_i$ is the variance of the $j^{th}$ feature of the samples in class $i$. The $j,k$ entry meanwhile is the covariance between the $j^{th}$ and $k^{th}$ features of the class $i$ samples.\n",
    "\n",
    "But remember, what we really care about are the variance between and within classes *once the data has been projected*. And we seek the projection that makes the ratio of the two as great as possible. Once projected, the mean of the projected features is in class $C_i$ is $y=\\mathbf{w}^T \\mathbf{\\mu_i}$. The variance of the projected samples, meanwhile, can be written as $y=\\mathbf{w}^T S_i \\mathbf{w}$. Let's take a look at why this is.\n",
    "\n",
    "$$\n",
    "y=\\mathbf{w}^T S_i \\mathbf{w} = \\frac{1}{N_i}\\sum_{\\mathbf{x} \\in C_i} \\mathbf{w}^T(\\mathbf{x} - \\mathbf{\\mu_i})(\\mathbf{x} - \\mathbf{\\mu_i})^T \\mathbf{w} = S_i \\mathbf{w} = \\frac{1}{N_i}\\sum_{\\mathbf{x} \\in C_i} \\left[\\mathbf{w}^T(\\mathbf{x} - \\mathbf{\\mu_i})\\right]^2 \\geq 0\n",
    "$$\n",
    "\n",
    "where the last step can be confirmed by noting that $\\mathbf{x} \\cdot \\mathbf{y} = \\mathbf{x}^T \\mathbf{y} = (\\mathbf{x}^T \\mathbf{y})^T = \\mathbf{y}^T \\mathbf{x}$ From this, we confirm that $S_i$ is positive definite, which implies the matrix is non-singular (a useful fact later). But why does this expression equal the variance of the projected samples? Let's do a little more rearranging...\n",
    "\n",
    "$$\n",
    "\\frac{1}{N_i}\\sum_{\\mathbf{x} \\in C_i} \\left[\\mathbf{w}^T(\\mathbf{x} - \\mathbf{\\mu_i})\\right]^2 = \\frac{1}{N_i}\\sum_{\\mathbf{x} \\in C_i} \\left[\\mathbf{w}^T\\mathbf{x} - \\mathbf{w}^T\\mathbf{\\mu_i}\\right]^2\n",
    "$$\n",
    "\n",
    "where $\\mathbf{w}^T\\mathbf{x}$ is our projected sample and $\\mathbf{w}^T\\mathbf{\\mu_i}$ is our projected mean. Thus, our expression is the sum of the squared deviations from the mean, which is just the variance!\n",
    "\n",
    "Now let's get an expression for the Variance Within Classes or \"within-class scatter\" of the projected samples. This value is simply the sum of the within-class variances for all classes $i$: $\\sum_{i} \\mathbf{w}^T S_i \\mathbf{w} = \\mathbf{w}^T (\\sum_{i}S_i) \\mathbf{w} = \\mathbf{w}^T S_W \\mathbf{w}$, where we pulled through the sum using the distributive property of matrix multiplication. For our example of simply two classes, we write this as $\\mathbf{w}^T S_0 \\mathbf{w} + \\mathbf{w}^T S_1 \\mathbf{w}$ or $\\mathbf{w}^T S_W \\mathbf{w}$ where $S_W = S_1 + S_2$.\n",
    "\n",
    "For the Variance between classes or \"between-class scatter\", we take a slightly different approach. We set $w$ such that the difference between the prjected means is large. We can write the distance between the means of the two classes as follows:\n",
    "\n",
    "$$\n",
    "\\left\\lVert \\mathbf{w}^T\\mathbf{\\mu_0} - \\mathbf{w}^T\\mathbf{\\mu_1}\\right\\rVert^2 = \\left\\lVert \\mathbf{w}^T(\\mathbf{\\mu_0} - \\mathbf{\\mu_1})\\right\\rVert^2 = \\left\\lVert (\\mathbf{\\mu_0} - \\mathbf{\\mu_1})^T \\mathbf{w}\\right\\rVert^2 = ((\\mathbf{\\mu_0} - \\mathbf{\\mu_1})^T \\mathbf{w})^T((\\mathbf{\\mu_0} - \\mathbf{\\mu_1})^T \\mathbf{w}) = \\mathbf{w}^T (\\mathbf{\\mu_0} - \\mathbf{\\mu_1}) (\\mathbf{\\mu_0} - \\mathbf{\\mu_1})^T \\mathbf{w}\n",
    "$$\n",
    "\n",
    "where we can view $(\\mathbf{\\mu_0} - \\mathbf{\\mu_1})(\\mathbf{\\mu_0} - \\mathbf{\\mu_1})^T$ as a between-class covariance matrix $S_B$. Therfore, the within-class scatter may be written more simply as $ \\mathbf{w}^T S_B \\mathbf{w}$\n",
    "\n",
    "Thus, we finally arrive at an expression for the separation $s(\\mathbf{w})$:\n",
    "\n",
    "$$\n",
    "s(\\mathbf{w}) = \\frac{\\mathbf{w}^T S_B \\mathbf{w}}{\\mathbf{w}^T S_W \\mathbf{w}} = \\frac{\\mathbf{w}^T (\\mathbf{\\mu_0} - \\mathbf{\\mu_1}) (\\mathbf{\\mu_0} - \\mathbf{\\mu_1})^T \\mathbf{w}}{\\mathbf{w}^T S_0 \\mathbf{w} + \\mathbf{w}^T S_1 \\mathbf{w}}\n",
    "$$\n",
    "\n",
    "where the last equality is for our simple 2 class example. To find the optimum $\\mathbf{w}$ we need to take gradient $\\nabla s(\\mathbf{w})$ and see where it vanishes. This matrix calculus can get pretty bad, so feel free to skip it, but I will try to provide a short intro in an aside.\n",
    "\n",
    "### Aside: Taking the gradient of $s(\\mathbf{w})$ ###\n",
    "\n",
    "To start, let's look at the gradient of the general expression $\\mathbf{x}^T \\mathbf{A} \\mathbf{x}$, where $\\mathbf{x}$ is an $n \\times 1$ vector, and $\\mathbf{A}$ is an $n \\times n$ matrix.\n",
    "\n",
    "We know that the quadratic form $\\mathbf{x}^T \\mathbf{A} \\mathbf{x}$ will equal some constant, let's call it $\\alpha$. Breaking apart the expression, we can get the following relation:\n",
    "\n",
    "$$\n",
    "\\alpha = \\mathbf{x}^T \\mathbf{A} \\mathbf{x} = \\mathbf{x}^T (\\mathbf{A} \\mathbf{x}) = \\sum_{i=1}^n x_i (\\mathbf{A} \\mathbf{x})_i = \\sum_{i=1}^n x_i \\sum_{j=1}^n a_{ij}x_j = \\sum_{i=1}^n \\sum_{j=1}^n a_{ij}x_ix_j\n",
    "$$\n",
    "\n",
    "We now differentiate $\\alpha$ with respect to the vector $\\mathbf{x}$, which will give us a $1 \\times n$ row vector. If this size confuses you, I highly suggest [this reference](https://atmos.washington.edu/~dennis/MatrixCalculus.pdf), which is an excellent resource on matrix calculus. The main point is that $\\frac{d\\mathbf{x}}{d\\alpha}$ is a column vector, while $\\frac{d\\alpha}{d\\mathbf{x}}$ is a row vector by convention. Let's carry out the differentiation below by taking the derivative with respect to one element of $\\mathbf{x}$, $x_k$:\n",
    "\n",
    "$$\n",
    "\\frac{d\\alpha}{d x_k} = \\frac{d}{dx_k}\\sum_{i=1}^n \\sum_{j=1}^n a_{ij}x_ix_j = \\sum_{i=1}^n a_{ik}x_i + \\sum_{j=1}^n a_{kj}x_j\n",
    "$$\n",
    "\n",
    "The summation term on the left  comes about by considering when $j=k$ and the summation term on the right comes about when $i=k$ is considered. Let's first focus on the left term $\\sum_{i=1}^n a_{ik}x_i$. This term is the dot product of the $k^{th}$ column of $\\mathbf{A}$ with $x$. And since we want our results to be row vectors, we can write it as the $k^{th}$ element of the row vector $(\\mathbf{x}^T \\mathbf{A})_k$. The second term $\\sum_{j=1}^n a_{kj}x_j$ is just the dot product of the $k^{th}$ row of $\\mathbf{A}$ with $\\mathbf{x}$. This is simply $(\\mathbf{A} \\mathbf{x})_k$, but since we want it as a row vector, we take the transpose to get $(\\mathbf{x}^T \\mathbf{A}^T)_k$.\n",
    "\n",
    "Thus we arrive at the following expression:\n",
    "\n",
    "$$\n",
    "\\frac{d\\alpha}{d x_k} = \\sum_{i=1}^n a_{ik}x_i + \\sum_{j=1}^n a_{kj}x_j = (\\mathbf{x}^T \\mathbf{A})_k + (\\mathbf{x}^T \\mathbf{A}^T)_k = (\\mathbf{x}^T \\mathbf{A} + \\mathbf{x}^T \\mathbf{A}^T)_k = (\\mathbf{x}^T (\\mathbf{A} + \\mathbf{A}^T))_k\n",
    "$$\n",
    "\n",
    "Or writing it for not just one element:\n",
    "\n",
    "$$\n",
    "\\frac{d \\mathbf{x}^T \\mathbf{A} \\mathbf{x}}{d \\mathbf{x}} = \\mathbf{x}^T (\\mathbf{A} + \\mathbf{A}^T)\n",
    "$$\n",
    "\n",
    "which just simplifies to $2\\mathbf{x}^T\\mathbf{A}$, if the matrix is symmetric (as our covariance matrices are).\n",
    "\n",
    "So now we are left to differentiate our expression as follows:\n",
    "\n",
    "$$\n",
    "\\frac{d s(\\mathbf{w})}{d\\mathbf{w}} = \\frac{d}{d\\mathbf{w}}\\frac{\\mathbf{w}^T S_B \\mathbf{w}}{\\mathbf{w}^T S_W \\mathbf{w}}\n",
    "$$\n",
    "\n",
    "Using the quotient rule:\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\mathbf{w}}\\frac{\\mathbf{w}^T S_B \\mathbf{w}}{\\mathbf{w}^T S_W \\mathbf{w}} = \\frac{(\\mathbf{w}^T S_B \\mathbf{w})(2\\mathbf{w}^T S_W) - (\\mathbf{w}^T S_W \\mathbf{w})(2\\mathbf{w}^T S_B)}{(\\mathbf{w}^T S_B \\mathbf{w})^2} \\stackrel{\\text{set}}{=}0 \\\\\n",
    "\\implies \\mathbf{w}^T S_W = \\left(\\frac{\\mathbf{w}^T S_W \\mathbf{w}}{\\mathbf{w}^T S_B \\mathbf{w}}\\right)\\mathbf{w}^T S_B\n",
    "$$\n",
    "\n",
    "To clean this expression up, I know that the ratio $\\frac{\\mathbf{w}^T S_W \\mathbf{w}}{\\mathbf{w}^T S_B \\mathbf{w}}$ is just some scalar I will call $K$. I will also take the transpose of the expression to get an equation for $\\mathbf{w}$ and not $\\mathbf{w}^T$. \n",
    "\n",
    "$$\n",
    "\\mathbf{w}^T S_W = K \\mathbf{w}^T S_B \\\\\n",
    "\\implies  S_W \\mathbf{w} = K  S_B \\mathbf{w}\\\\\n",
    "\\implies  \\mathbf{w} = K  S^{-1}_W S_B \\mathbf{w}\\\\\n",
    "\\text{For two classes:}\n",
    "\\implies \\mathbf{w} = K  (S_1 + S_2)^{-1} (\\mathbf{\\mu_0} - \\mathbf{\\mu_1})(\\mathbf{\\mu_0} - \\mathbf{\\mu_1})^T \\mathbf{w}\\\\\n",
    "$$\n",
    "\n",
    "We recognize that $(\\mathbf{\\mu_0} - \\mathbf{\\mu_1})^T \\mathbf{w}$ is also a scalar that can be absorbed into the constant, and finally discover the direction of $w$:\n",
    "\n",
    "$$\n",
    "w \\propto (S_1 + S_2)^{-1} (\\mathbf{\\mu_0} - \\mathbf{\\mu_1})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
